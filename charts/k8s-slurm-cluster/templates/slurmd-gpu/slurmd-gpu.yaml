apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Values.slurmgpu.name }}
  creationTimestamp: null
  labels:
    app.kubernetes.io/name: slurm
    app.kubernetes.io/component: slurmgpu
spec:
  replicas: {{ .Values.slurmgpu.replicas }}
  selector:
    matchLabels:
      app.kubernetes.io/name: slurm
      app.kubernetes.io/component: slurmgpu
  serviceName: slurmgpu
  template:
    metadata:
      creationTimestamp: null
      {{- if eq .Values.network.rdma "true" }}
      {{- if .Values.network.ipoibnetworks }}
      annotations:
        {{- range $.Values.network.ipoibnetworks }}
        k8s.v1.cni.cncf.io/networks: {{ . }}
        {{- end }}
      {{- end }}
      {{- end }}
      labels:
        app.kubernetes.io/name: slurm
        app.kubernetes.io/component: slurmgpu
{{- if .Values.slurmgpu.labels }}
{{ toYaml .Values.slurmgpu.labels | indent 8 }}
{{- end }}
    spec:
      {{- with .Values.slurmgpu.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- with .Values.slurmgpu.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
      {{- end }}
      {{- with .Values.slurmgpu.nodeAffinity }}
      affinity:
{{ toYaml . | indent 8 }}
      {{- end }}
      terminationGracePeriodSeconds: {{ .Values.slurmgpu.terminationGracePeriod }}
      initContainers:
      {{- if eq .Values.auth.krb5.enabled "true" }}
      - name: ad-join-init-container
        image: ubiquitycluster/slurm-adcli:latest
        command: ["/bin/sh", "-c"]
        args:
        - |
          echo "Reading SSSD credentials from secret..."
          echo $username
          echo $password
      {{- end }}
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -n 131072"]
        securityContext:
          privileged: true
      - name: increase-locked-memory-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -l unlimited"]
        securityContext:
          privileged: true
      - name: increase-stacksize-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -s unlimited"]
        securityContext:
          privileged: true
      containers:
        - name: slurmgpu
          image: {{ .Values.slurmgpu.image }}
          imagePullPolicy: {{ .Values.slurmgpu.imagePullPolicy }}
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
{{- with .Values.slurmgpu.env }}
{{ toYaml . | indent 12 }}
{{- end }}
          ports:
            - containerPort: 6818
              hostPort: 6818
          {{- if eq .Values.network.rdma "true" }}
          resources:
            requests:
{{ toYaml .Values.slurmgpu.resources.requests | indent 14 }}
              rdma/rdma_shared_device_a: 1
            limits:
{{ toYaml .Values.slurmgpu.resources.limits | indent 14 }}
              rdma/rdma_shared_device_a: 1
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_ADMIN
          {{- else }}
          resources:
{{ toYaml .Values.slurmgpu.resources | indent 12 }}
          securityContext:
            privileged: true
            capabilities:
               add:
               - SYS_ADMIN
          {{- end }}
          volumeMounts:
            - name:  {{ .Values.configmaps.sssdConfig }}
              mountPath: /etc/sssd/sssd.conf
              subPath: sssd.conf
            - name: {{ .Values.configmaps.slurmProlog }}
              mountPath: /etc/slurm/prolog.d
            - name: {{ .Values.configmaps.slurmEpilog }} 
              mountPath: /etc/slurm/epilog.d
            - name: {{ .Values.configmaps.slurmMailProg }}
              mountPath: {{ .Values.slurmctld.config.slurm.MailProg.location }}
              subPath: mailprog.sh
            - name: slurm-config-volume
              mountPath: /etc/slurm/slurm.conf
              subPath: slurm.conf
            - name: slurm-config-cgroup
              mountPath: /etc/slurm/cgroup.conf
              subPath: cgroup.conf
            - name: sys-fs-cgroup
              mountPath: /sys/fs/cgroup
            - name: run-dbus
              mountPath: /var/run/dbus
            - name: slurm-jobdir
              mountPath: {{ .Values.storage.mountPath }}
            - name: munge-key-secret
              mountPath: /tmp/munge.key
              subPath: munge.key
            {{- if eq .Values.auth.krb5.enabled "true" }}
            - name: keytab-volume
              mountPath: /etc/krb5.keytab
              subPath: keytab
            {{- end }}
            {{- if .Values.slurmgpu.volumeMounts }}
{{ toYaml .Values.slurmgpu.volumeMounts | indent 12 }}
        {{- end }}
      dnsPolicy: ClusterFirstWithHostNet
      dnsConfig:
        searches:
          - slurmgpu.hpc-ubiq.svc.cluster.local
      restartPolicy: Always
      volumes:
        {{- if eq .Values.auth.krb5.enabled "true" }}
        - name: sssd-creds
          secret:
            secretName: sssd-creds
        {{- end }}
        - name: sys-fs-cgroup
          hostPath:
            path: /sys/fs/cgroup
        - name: run-dbus
          hostPath:
            path: /var/run/dbus
        - name: {{ .Values.configmaps.sssdConfig }}
          configMap:
            name: {{ .Values.configmaps.sssdConfig }}
            defaultMode: 0600
        - name: {{ .Values.configmaps.slurmProlog }}
          configMap:
            name: {{ .Values.configmaps.slurmProlog }}
            defaultMode: 0555
        - name: {{ .Values.configmaps.slurmEpilog }}
          configMap:
            name: {{ .Values.configmaps.slurmEpilog }}
            defaultMode: 0555
        - name: {{ .Values.configmaps.slurmMailProg }}
          configMap:
            name: {{ .Values.configmaps.slurmMailProg }}
            defaultMode: 0555
        - name: slurm-config-cgroup
          configMap:
            name: {{ .Values.configmaps.slurmCgroupConf }}
        - name: slurm-jobdir
          persistentVolumeClaim:
            claimName: {{ .Values.storage.claimName }}
        - name: slurm-config-volume
          configMap:
            name: {{ .Values.configmaps.slurmConf }}
        - name: keytab-volume
          emptydir: {}
        - name: munge-key-secret
          secret:
            secretName: {{ .Values.secrets.mungeKey }}
            defaultMode: 0400
        {{- if .Values.slurmgpu.volumes }}
{{ toYaml .Values.slurmgpu.volumes | indent 8 }}
        {{- end }}
